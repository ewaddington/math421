d = read.csv('https://bryantstats.github.io/math461/slides/heart_data.csv')
my_linear_model <- lm(heart_disease ~ biking, data = d)
summary(my_linear_model)
my_linear_model <- lm(heart_disease ~ biking + smoking, data = d)
summary(my_linear_model)
my_linear_model <- lm(heart_disease ~ biking, data = d)
summary(my_linear_model)
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
model = lm(TOTCHG~AGE + LOS, data = d)
summary(model)
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
D
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
d
model = lm(TOTCHG~AGE + LOS, data = d)
summary(model)
my_summary=summary(model)
plot(model)
predict(model, list(AGE = 13, LOS = 7))
predict(model, list(AGE = 13, LOS = 14))
predict(model, list(AGE = 21, LOS = 14))
full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
predict(full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
)
full_model <- lm(TOTCHG~ ., data = d)
View(full_model)
full_model12 <- lm(TOTCHG~ ., data = d)
predict(model, list(AGE = 13, LOS = 7))
predict(model, list(AGE = 21, LOS = 14))
p=1/1+e^-1.530-.735+.031
p=1/1+exponential^-1.530-.735+.031
p=1/1+exp(-1.530-.735+.031)
p=1/1+exp(-1.530-.735+.031)
p=1/1+exp(-1.485+1.175+.101)
p=1/(1+exp(-1.485+1.175+.101))
p
p=1/(1+exp(-1.530-.735+.031))
p
p=1/(1+exp(-.05+.04*10-2+1.20*780))
p
p=1/(1+exp(-.05+(.04*10)-(1*2)+(1.20*780)))
p
p=1/(1+exp(-.05+.4-2+936))
p
p=1/(1+exp(-.05+(.04*10)-(1*2)+(1.20*.78)))
p
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
model = lm(TOTCHG~AGE + LOS, data = d)
summary(model)
full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
reduced_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) , data = d)
anova(reduced_model, full_model)
full_model <- lm(TOTCHG~ LOS + factor(GENDER) + factor(RACE) + AGE + APRDRG, data = d)
reduced_model
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
d
View(d)
median_cost = median(d$TOTCHG)
median_cost
View(d)
d$charge = ifelse(d$TOTCHG < median_cost, 0, 1)
model1 = glm(charge ~ AGE + LOS,
data = d,
family = binomial(link = "logit"))
# Report model summary
summary(model)
predicted_prob = predict(model, list(AGE = 20, LOS = 1),
type = 'response')
predicted_prob
predicted_prob1
predicted_prob
predicted_class = ifelse(predicted_prob>=.5, 1, 0)
predicted_class
predicted_class
d = read.csv('poisson_sim2.csv')
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
View(d)
median_cost=median(dTOTCHG)
median_cost=median(dSTOTCHG)
median_cost
dScharge=ifelse(dScharge<median_cost,0,1)
charge=ifelse(charge<median_cost,0,1)
d = read.csv('https://bryantstats.github.io/math461/assignments/HospitalCosts.csv')
# Calculate median charge
median_cost = median(d$TOTCHG)
# Create a column to compare the cost with median charge
# this column takes value 0 of the charge is less than the median and 1 otherwise
d$charge = ifelse(d$TOTCHG < median_cost, 0, 1)
# Train
model = glm(charge ~ AGE + LOS,
data = d,
family = binomial(link = "logit"))
# Report model summary
summary(model)
# Train
model = glm(charge ~ AGE + LOS,
data = d,
family = binomial(link = "logit"))
# Report model summary
summary(model)
predicted_prob = predict(model, list(AGE = 20, LOS = 1),
type = 'response')
predicted_prob
predicted_class = ifelse(predicted_prob>=.5, 1, 0)
predicted_class
# Create
d$predicted_prob_charge = predict(model, d, type = "response")
d$predicted_charge = ifelse(d$predicted_prob_charge <.5, 0, 1)
mean(d$predicted_charge == d$charge)
# Create
d$LOS2 = ifelse(d$LOS2 <median(los), 0, 1)
d$LOS2 = ifelse(d$LOS2 <1536.5, 0, 1)
# Create
library(tidyverse)  # data manipulation
library(tidyverse)  # data manipulation
df = read.csv('autos.csv')
library(tidyverse)  # data manipulation
stdev(.57,.82,1.45,1.87,1.46)
q()
knitr::opts_chunk$set(echo = TRUE)
# Modeling w/ Numeric Targets
# Linear Regression
# Linear Regression estimates the linear model between the target variable and the explanatory variables.
df = read.csv("water_potability.csv")
df = df %>%
dplyr::select(Solids, Hardness, ph, Sulfate, Chloramines, Organic_carbon) %>%
rename(target = Hardness) %>%
drop_na()
# QDA
# Quadratic Discriminant Analysis is an extension of LDA, however it does not use a linear model, therefore it is capable of providing a better fit to the data.
model = qda(target ~ Hardness + ph,
data = df)
# Unsupervised Techniques
# Logistic Regression
library(tidyverse)
library(caret)
df = read_csv('water_potability.csv')
df = df %>%
select(Potability, ph, Hardness, Sulfate) %>%
drop_na()
median_potability = median(df$Potability)
df$target = ifelse(df$Potability > median_potability, "0", "1")
df$Potability = NULL
df = df %>%
mutate(ph = factor(ph),
Hardness = factor(Hardness),
Sulfate = factor(Sulfate),
target = factor(target))
model = glm(target ~ .,
data = df,
family = binomial(link = "logit"))
# Modelling w/ Binary Targets
# GLM
# Generalized Linear Models use logistic regressions to model the outcome of a target variable using binomial distributions. It creates a probability of success for the variables used as predictors.
d = read_csv("water_potability.csv") %>% drop_na()
model = glm(Potability ~ ph + Hardness,
data = d,
family = binomial(link = "logit"))
summary(model)
predicted_prob = predict(model, list(ph = 2, Hardness = 150),
type = 'response')
predicted_prob
predicted_class = ifelse(predicted_prob>=.5, 1, 0)
predicted_class
# LDA
# Linear Discriminant Analysis categorizes data into groups by finding a linear combination of variables.
library(caret)
library(tidyverse)
library(MASS)
df = read_csv("water_potability.csv")
df= df %>%  rename(target = Potability)
model = lda(target ~ Hardness + ph,
data = df)
pred = predict(model, df,
type = 'response')$class
cm <- confusionMatrix(data = pred, reference = factor(df$target))
cm$overall[1]
d = data.frame(pred = pred, obs = factor(df$target))
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
# QDA
# Quadratic Discriminant Analysis is an extension of LDA, however it does not use a linear model, therefore it is capable of providing a better fit to the data.
model = qda(target ~ Hardness + ph,
data = df)
pred = predict(model, df,
type = 'response')$class
cm <- confusionMatrix(data = pred, reference = factor(df$target))
cm$overall[1]
# Modeling w/ Numeric Targets
# Linear Regression
# Linear Regression estimates the linear model between the target variable and the explanatory variables.
df = read.csv("water_potability.csv")
df = df %>%
dplyr::select(Solids, Hardness, ph, Sulfate, Chloramines, Organic_carbon) %>%
rename(target = Hardness) %>%
drop_na()
my_linear_model <- lm(target ~ ., data = df)
summary(my_linear_model)
my_linear_model <- lm(target ~., data = df)
summary(my_linear_model)
# PCA Regression
# Principal Component Analysis used in regression is used for estimating explanatory variables in a linear regression model.
res.pca <- prcomp(df_pca,  scale = TRUE)
library(tidyverse)
library(caret)
df = read_csv('water_potability')
library(tidyverse)
library(caret)
# Select variables for modeling
df = df %>%
select(Potability, ph, Hardness, Sulfates) %>%
drop_na()
library(tidyverse)
library(caret)
df = read_csv('water_potability.csv')
# Select variables for modeling
df = df %>%
select(Potability, ph, Hardness, Sulfates) %>%
drop_na()
# Select variables for modeling
df = df %>% select(Potability, ph, Hardness, Sulfates) %>%
drop_na()
# LDA
# Linear Discriminant Analysis categorizes data into groups by finding a linear combination of variables.
library(caret)
library(tidyverse)
library(MASS)
df = read_csv("water_potability.csv")
df= df %>%  rename(target = Potability)
model = lda(target ~ Hardness + ph,
data = df)
pred = predict(model, df,
type = 'response')$class
cm <- confusionMatrix(data = pred, reference = factor(df$target))
cm$overall[1]
d = data.frame(pred = pred, obs = factor(df$target))
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
# QDA
# Quadratic Discriminant Analysis is an extension of LDA, however it does not use a linear model, therefore it is capable of providing a better fit to the data.
model = qda(target ~ Hardness + ph,
data = df)
pred = predict(model, df,
type = 'response')$class
cm <- confusionMatrix(data = pred, reference = factor(df$target))
cm$overall[1]
d = data.frame(pred = pred, obs = factor(df$target))
library(yardstick)
d %>% conf_mat(pred, obs) %>% autoplot
# Modeling w/ Numeric Targets
# Linear Regression
# Linear Regression estimates the linear model between the target variable and the explanatory variables.
df = read.csv("water_potability.csv")
df = df %>%
dplyr::select(Solids, Hardness, ph, Sulfate, Chloramines, Organic_carbon) %>%
rename(target = Hardness) %>%
drop_na()
my_linear_model <- lm(target ~ ., data = df)
summary(my_linear_model)
my_linear_model <- lm(target ~., data = df)
summary(my_linear_model)
# PCA Regression
# Principal Component Analysis used in regression is used for estimating explanatory variables in a linear regression model.
res.pca <- prcomp(df_pca,  scale = TRUE)
get_eig(res.pca)
# PCA Regression
# Principal Component Analysis used in regression is used for estimating explanatory variables in a linear regression model.
df_pca = df %>% dplyr::select(-target)
res.pca <- prcomp(df_pca,  scale = TRUE)
df2 = as_tibble(res.pca$x[, c(1:2)])
df2$target = df$target
df = df2
model = lm(target~., data = df)
summary(model)
# GLM Poisson Regression
# Poisson regression uses a generalized linear model to create a Poisson distribution to model the target variable using explanitory variables.
model = glm(target ~. ,
data = df,
family = poisson(link = "log"))
r2 = with(summary(model), 1 - deviance/null.deviance)
print(paste0("R-Squared: ", r2))
predict(model,
list(Hardness = 15, ph = 4),
type = 'response')
# Goodness-of-fit test
gof.pvalue = 1 - pchisq(model$deviance, model$df.residual)
gof.pvalue
knitr::opts_chunk$set(message = FALSE)
install.packages("gganimate")
install.packages("gifski")
setwd("C:/Users/student/Desktop/New folder/math_421")
knitr::opts_chunk$set(message = FALSE)
library(ggplot2)
library(gganimate)
adult_data <- read.csv("adult_census_missing.csv")
adult_data$education_num <- as.numeric(as.character(adult_data$education_num))
library(ggplot2)
library(gganimate)
adult_data <- read.csv("adult_census_missing.csv")
adult_data$education.num <- as.numeric(as.character(adult_data$education.num))
p <- ggplot(adult_data, aes(x = age, y = hours_per_week, color = education)) +
geom_point(alpha = 0.7, size = 2) +
labs(title = "Age vs Hours Worked Per Week by Education Level: {closest_state}",
x = "Age", y = "Hours Worked Per Week") +
theme_minimal() +
transition_states(education, transition_length = 2, state_length = 1)
p <- ggplot(adult_data, aes(x = age, y = hours_per_week, color = education)) +
geom_point(alpha = 0.7, size = 2) +
labs(title = "Age vs Hours Worked Per Week by Education Level: {closest_state}",
x = "Age", y = "Hours Worked Per Week") +
theme_minimal() +
transition_states(education, transition_length = 2, state_length = 1)
library(ggplot2)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(knitr)
adult_data <- read.csv("adult_census_missing.csv")
adult_data$education.num <- as.numeric(as.character(adult_data$education.num))
df %>% ggplot(aes(x = Sex,
fill=income))+
geom_bar(position = 'fill')+
transition_states(education)
df %>% ggplot(aes(x = sex,
fill=income))+
geom_bar(position = 'fill')+
transition_states(education)
df %>% ggplot(aes(x = education,
fill=sex))+
geom_bar(position = 'fill')+
transition_states(income)
df %>% ggplot(aes(x = 'education',
fill='sex'))+
geom_bar(position = 'fill')+
transition_states('income')
library(ggplot2)
library(gganimate)
library(ggplot2)
library(tidyverse)
library(knitr)
adult_data <- read.csv("adult_census_missing.csv")
adult_data$education.num <- as.numeric(as.character(adult_data$education.num))
df %>% ggplot(aes(x = education.num,
y = income, color=Survived))+
geom_point()+
transition_states(race)+
labs(title = 'Race: {closest_state}')
df %>% ggplot(aes(x = education.num,
y = income, color=Survived))+
geom_point()+
transition_states(race)+
labs(title = 'Race: {closest_state}')
df %>% ggplot(aes(x = 'education.num',
y = 'income', color='sex'))+
geom_point()+
transition_states('race')+
labs(title = 'Race: {closest_state}')
library(ggplot2)
library(gganimate)
library(dplyr)
read.csv("adult_census_missing.csv")
p <- ggplot(data, aes(x = age, y = hours.per.week, color = income)) +
geom_point() +
transition_states(income, transition_length = 2, state_length = 1) +
labs(title = 'Income: {closest_state}', x = 'Age', y = 'Hours per Week')
p <- ggplot(data, aes(x = 'age', y = 'hours.per.week', color = 'income')) +
geom_point() +
transition_states(income, transition_length = 2, state_length = 1) +
labs(title = 'Income: {closest_state}', x = 'Age', y = 'Hours per Week')
p <- ggplot(data, aes(x = age, y = hours.per.week, color = income)) +
geom_point(alpha = 0.7) +
transition_states(income, transition_length = 2, state_length = 1) +
labs(
title = "Income Group: {closest_state}",
x = "Age",
y = "Hours per Week"
) +
theme_minimal()
install.packages("gganimate")
install.packages("gifski")
